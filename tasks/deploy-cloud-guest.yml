- name: == Cloud Image == Destroying Virtual Guest 
  virt: 
    name: "{{ inventory_hostname }}" 
    state: destroyed
  ignore_errors: yes
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == Undefining Virtual Guest 
  virt: 
    name: "{{ inventory_hostname }}" 
    command: undefine
  ignore_errors: yes
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == undefining UEFI VMs with nvram
  shell: "virsh undefine --nvram {{ inventory_hostname }}"
  when: '"aarch64" in inventory_hostname'
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == Creating structure for cloud-init nocloud 
  file:
    path: "/var/lib/libvirt/images/cloud-configs/{{ inventory_hostname }}"
    state: directory
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == distributing cloud-init config files
  template:
    src: "../templates/cloud-init/{{ item }}.j2"
    dest: "/var/lib/libvirt/images/cloud-configs/{{ inventory_hostname }}/{{ item }}"
  with_items:
    - meta-data
    - user-data
    - network-config
  delegate_to: "{{ kvm_host }}"

- name: == Cloud image == Generating cloud-init iso image
  command: "genisoimage -output {{ inventory_hostname }}-metadata.iso -volid cidata -joliet -r meta-data user-data network-config"
  args:
    chdir: "/var/lib/libvirt/images/cloud-configs/{{ inventory_hostname }}/"
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == Downloading qcow2 cloud image
  get_url:
    url: "{{ cloud_image_url }}"
    dest: "/var/lib/libvirt/images/{{ inventory_hostname }}.qcow2"
    force: yes
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == Creating virt-install import command
  template: 
    src: "../templates/ansible-virt-import-cloud.j2" 
    dest: "/var/lib/libvirt/virt-import-{{ inventory_hostname }}" 
    mode: 0750 
    owner: root 
    group: root
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == Provisining the Virtual Guest[s]
  command: /var/lib/libvirt/virt-import-{{ inventory_hostname }}
  delegate_to: "{{ kvm_host }}"

- name: == Cloud Image == Waiting for sshd to be available on the deployed node
  wait_for: 
    port: 22 
    host: "{{ ip }}" 
    timeout: 1200 
  delegate_to: "{{ deploy_node }}"

   
    
